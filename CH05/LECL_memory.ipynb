{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCEL (ëŒ€í™”ë‚´ìš© ê¸°ì–µí•˜ê¸°): ë©”ëª¨ë¦¬ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ì˜ì˜ ì²´ì¸ì— ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ì¬ ë©”ëª¨ë¦¬ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ ìˆ˜ë™ìœ¼ë¡œ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4yvbq7s16359fdr947szsx5h0000gn/T/ipykernel_5879/3650269879.py:8: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma3:1b\")\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:1b\")\n",
    "\n",
    "# ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì´ì „ ëŒ€í™” ë‚´ì—­, ê·¸ë¦¬ê³  ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëŒ€í™”ë‚´ìš©ì„ ì €ì¥í•  ë©”ëª¨ë¦¬ì¸ ConversationBufferMemory ìƒì„±í•˜ê³  return_messages ë§¤ê°œë³€ìˆ˜ë¥¼ Trueë¡œ ì„¤ì •í•˜ì—¬, ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ê°€ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4yvbq7s16359fdr947szsx5h0000gn/T/ipykernel_5879/2260825137.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "# ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ë©”ì‹œì§€ ë°˜í™˜ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})  # ë©”ëª¨ë¦¬ ë³€ìˆ˜ë¥¼ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables)\n",
    "    | itemgetter(\"chat_history\")  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi', 'chat_history': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'hi', 'chat_history': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"hi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ì„ ë•ê¸° ìœ„í•´ ì—¬ê¸° ìˆëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# chain ê°ì²´ì˜ invoke ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "response = chain.invoke({\"input\": \"ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš°ì…ë‹ˆë‹¤.\"})\n",
    "print(response.content)  # ìƒì„±ëœ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memory.save_context í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°(inputs)ì™€ ì‘ë‹µ ë‚´ìš©(response.content)ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” AI ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ í˜„ì¬ ìƒíƒœë¥¼ ê¸°ë¡í•˜ê±°ë‚˜, ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ì‹œìŠ¤í…œì˜ ì‘ë‹µì„ ì¶”ì í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš°ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ì„ ë•ê¸° ìœ„í•´ ì—¬ê¸° ìˆëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš°ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ê¸°ì–µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì €ëŠ” ë‹¹ì‹ ê³¼ ëŒ€í™”í•˜ëŠ” ë°ë§Œ ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ˜Š', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì…ë ¥ëœ ë°ì´í„°ì™€ ì‘ë‹µ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "memory.save_context(\n",
    "    {\"human\": \"ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš°ì…ë‹ˆë‹¤.\"}, {\"ai\": response.content}\n",
    ")\n",
    "\n",
    "# ì €ì¥ëœ ëŒ€í™”ê¸°ë¡ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„, ê¸°ì–µë‚¬ì–´ìš”! ë‹¹ì‹ ì˜ ì´ë¦„ì€ í˜„ìš°ì˜€ì£ . ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ ì¶”ê°€ ì§ˆì˜í•©ë‹ˆë‹¤.\n",
    "response = chain.invoke({\"input\": \"ì œ ì´ë¦„ì´ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ ê¸°ì–µí•˜ì„¸ìš”?\"})\n",
    "# ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì»¤ìŠ¤í…€ ConversationChain êµ¬í˜„ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Ollama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "llm = ChatOllama(model=\"gemma3:1b\")\n",
    "\n",
    "# ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì´ì „ ëŒ€í™” ë‚´ì—­, ê·¸ë¦¬ê³  ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ë©”ì‹œì§€ ë°˜í™˜ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConversationChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory, input_key=\"input\"):\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables)\n",
    "                | itemgetter(memory.memory_key)  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({self.input_key: query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_chain = MyConversationChain(llm, prompt, memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜! ë§Œë‚˜ì„œ ì •ë§ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ì„ ë•ê¸° ìœ„í•´ ì—¬ê¸° ìˆëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š \\n\\n(Hello, Hyunwoo! It's really nice to meet you. I am an AI chatbot here to help you. What can I do for you?)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì•ˆë…•í•˜ì„¸ìš”? ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš° ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì œ ì´ë¦„ì€ ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ˜Š'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì œ ì´ë¦„ì´ ë­ë¼ê³ ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤. \\n\\n(Yes, I understand. From now on, I will only answer in English.)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš” ì•Œê² ì–´ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜. ğŸ˜Š'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì œ ì´ë¦„ì„ ë‹¤ì‹œ í•œ ë²ˆ ë§í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”? ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í˜„ìš° ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜! ë§Œë‚˜ì„œ ì •ë§ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì €ëŠ” ì—¬ëŸ¬ë¶„ì„ ë•ê¸° ìœ„í•´ ì—¬ê¸° ìˆëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š \\n\\n(Hello, Hyunwoo! It's really nice to meet you. I am an AI chatbot here to help you. What can I do for you?)\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì œ ì´ë¦„ì´ ë­ë¼ê³ ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì œ ì´ë¦„ì€ ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ˜Š', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš” ì•Œê² ì–´ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤. \\n\\n(Yes, I understand. From now on, I will only answer in English.)', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì œ ì´ë¦„ì„ ë‹¤ì‹œ í•œ ë²ˆ ë§í•´ì£¼ì„¸ìš”', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í˜„ìš°ë‹˜. ğŸ˜Š', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
